{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Copyright\n",
    "\n",
    "I'm wrote and shared this code to help my fellow students with the Data Engineering projects, because a \n",
    "lot of student are having difficulties with it. Please do not see this as an opportunity however of just copying my\n",
    "code and handing it in as your own with little adjustments. \n",
    "\n",
    "Do feel free however to copy code snippets to incorporate into your own work. That's how programming works so it's even highly encouraged!\n",
    "Please do cite my work in that case: \"Heymans Jonas, (2020), https://github.com/JonasHeymans/de_extra\". Once at the top of your file is enough, \n",
    "you do not need to go into the effort of putting my name after every line of code you copy.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "We run a webshop that sells all kinds of products and where customers are able to write reviews about those items that they buy. We want to monitor the number of reviews by customers over time so that we can give incentives to do so in case these numbers diminish drastically.\n",
    "\n",
    "The goal of this project is to compute summary statistics from tab separated files that contain reviews for Amazon products. The summary statistics should be stored in a database and displayed on a dashboard to see the evolution of the number of reviews per month.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Caveat\n",
    "\n",
    "Still (major) bugs in the code. Use responsibly and stay critical. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mysql.connector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "b'java version \"1.8.0_251\"\\nJava(TM) SE Runtime Environment (build 1.8.0_251-b08)\\nJava HotSpot(TM) 64-Bit Server VM (build 25.251-b08, mixed mode)\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Yeah, don't mind this. It is just for myself to check if I have the correct java version activated.\n",
    "\n",
    "source ~/.bashrc\n",
    "source ~/.bash_profile\n",
    "\n",
    "uncomment the following line in /.bash_profile\n",
    " - export JAVA_HOME=$(/usr/libexec/java_home -v 1.8) pyspark\n",
    "'''\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home'\n",
    "\n",
    "import subprocess\n",
    "subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/jonas/PycharmProjects/de_extra/opdracht_amazon/amazon_reviews_multilingual_US_v1_00.tsv;'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o61.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/jonas/PycharmProjects/de_extra/opdracht_amazon/amazon_reviews_multilingual_US_v1_00.tsv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-d3176d11de65>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mamazon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'amazon_reviews_multilingual_US_v1_00.tsv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'\\t'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0minferSchema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001B[0m\n\u001B[1;32m    474\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    475\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 476\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    477\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    478\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     67\u001B[0m                                              e.java_exception.getStackTrace()))\n\u001B[1;32m     68\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'org.apache.spark.sql.AnalysisException: '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mAnalysisException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'org.apache.spark.sql.catalyst.analysis'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mAnalysisException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: 'Path does not exist: file:/Users/jonas/PycharmProjects/de_extra/opdracht_amazon/amazon_reviews_multilingual_US_v1_00.tsv;'"
     ]
    }
   ],
   "source": [
    "amazon = spark.read.csv('../amazon_reviews_multilingual_US_v1_00.tsv', sep='\\t', header=True,inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#As you can see I've commented out a lot of the .show(), .printSchema(),... code. I do this \n",
    "#because it takes a long time to run and I don't want to wait 2 full minutes every time \n",
    "#I run the code. I highly recommend however that you uncomment it and run it yourself when \n",
    "#you are reading through the code so that you understand why we do each operation.\n",
    "\n",
    "# amazon.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We know from the project description that we only need the \n",
    "# customer_id, review_date, product_category and star_rating columns, \n",
    "# so we'll drop all the other columns.\n",
    "amazon = amazon.select('customer_id', 'review_date', 'product_category', 'star_rating')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# amazon.printSchema()\n",
    "\n",
    "'''\n",
    "So there is no problem with our datatypes: Spark doesn't do weird things like\n",
    "saying that a column full of numbers is of type string or something like that.\n",
    "If it does, you can use this code to tell spark, for example, that the \n",
    "customer_id-column has the type 'int'\n",
    "\n",
    "amazon = amazon.withColumn('customer_id', amazon['customer_id'].cast('int'))\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# amazon.select('review_date').show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Extracting the month from the 'review_date' column , creating a new column\n",
    "called \"month\" and deleting the \"review_date\" column. \n",
    "Please note that this month column is in numerical format (E.g. 1 = \"January\")\n",
    "'''\n",
    "from pyspark.sql.functions import month\n",
    "amazon = amazon.withColumn(\"month\", month(amazon['review_date']))\n",
    "amazon = amazon.drop('review_date')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# amazon.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Ok, here it gets messy. It is also coded quite dirty but YOLO.\n",
    "\n",
    "What we do here is important however. In the first paragraph we make a \n",
    "new dataframe called 'amazon_grouped' in which we store the data from \n",
    "our amazon dataframe, but also add a new column called 'count'. In this\n",
    "new column we store how many times each unique combination of \n",
    "'month','product_category' and 'star_rating appears in amazon. \n",
    "\n",
    "So, as you can see when you run amazon_grouped.show() \"month = 1,\n",
    "product_category = Apparel and star_rating = 1\" appears only one\n",
    "time in amazon, while \"month = 1, product_category = Apparel and \n",
    "star_rating = 5\" appears no less than 9 times! \n",
    "(Yay, people are happy with the clothes they buy at amazon in January. \n",
    "Probably because they just got new money thanks to selling the sweaters \n",
    "and the Mamma Mia Blu-ray they got as a present from grandma, fucking \n",
    "ungrateful shits)\n",
    "\n",
    "In the second paragraph we do the same, but now only for the unique \n",
    "combinations of 'month' and'product_category'. Which, as you might\n",
    "have guessed, gives us way less rows and higher values in the counts-column,\n",
    "which I renamed here to \"count_per_month\"\n",
    "\n",
    "In the last paragraph we add both columns together to make our \n",
    "final_dataframe.\n",
    "\n",
    "If you don't understand this part completely, no worries: I'll explain it\n",
    "to you in the call and you'll see that is actually quite easy, pinky promise! \n",
    "\n",
    "'''\n",
    "\n",
    "feature_group = ['month','product_category','star_rating']\n",
    "amazon_grouped = amazon.groupBy(feature_group).count()\n",
    "# print('amazon_grouped')\n",
    "# amazon_grouped.sort('product_category','month','star_rating').show()\n",
    "\n",
    "per_month = amazon.groupBy('month','product_category').count()\n",
    "per_month = per_month.withColumnRenamed('count', \"count_per_month\")\n",
    "# print('per_month')\n",
    "# per_month.sort('product_category','month').show()\n",
    "\n",
    "final_dataframe = amazon_grouped.join(per_month, on= ['month','product_category'], how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('final_dataframe')\n",
    "# final_dataframe.sort('product_category','month','star_rating').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Comment these lines if you don't want to get Rick rolled and learn the danger of running \n",
    "code without understanding what it does or reading the documentation.\n",
    "'''\n",
    "\n",
    "import webbrowser\n",
    "webbrowser.open('https://www.youtube.com/watch?v=dQw4w9WgXcQ') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It is bad practice to hardcode your passwords into your programs. \n",
    "# That's why we prompt the user here to type it in himself every time the program runs\n",
    "database_password = input(\"Can I have your password please?\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting up a connection with our local database and creating a cursor\n",
    "\n",
    "cnx = mysql.connector.connect(\n",
    "    host = '127.0.0.1',\n",
    "    user = 'root',\n",
    "    passwd = database_password,\n",
    "    auth_plugin='mysql_native_password',\n",
    "    buffered=True\n",
    ")\n",
    "\n",
    "my_cursor = cnx.cursor()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we run the \"amazon.sql\" file, in which we set up our database and\n",
    "# insert a table into it.\n",
    "path = \"amazon.sql\"\n",
    "for line in open(path).read().split(';'):\n",
    "    my_cursor.execute(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the documentation to know what this method does you lazy fuck! \n",
    "\n",
    "collect = final_dataframe.collect()\n",
    "\n",
    "'''\n",
    "Ok, I'll help because who in the world reads documentation: \n",
    "collect() returns all the records as a list of Row. \n",
    "\n",
    "So we'll get a list of all the records in this shape: \n",
    "Row(month=8, product_category='Video', star_rating=1, count=286, count_per_month=5074 )\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = final_dataframe.count()\n",
    "print(total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Ok, this is another hard part (but not really once you get it). I could again\n",
    "explain here every line and show what it does, but that would result in a \n",
    "wall of text and also I'm getting bored. So let's give this a didactic spin \n",
    "(which has nothing to do with the fact that the sun is shining and I want to \n",
    "go outside) and say that you can try and figure it out yourself as an exercise.\n",
    "Yes, let's say that: it's an exercise... \n",
    "\n",
    "'''\n",
    "for x in range (total):\n",
    "    \n",
    "    # Extracting the month of the x'th row and saving it to a variable called \n",
    "    # 'month' (and the same for the other variables)\n",
    "    month = collect[x][0]\n",
    "    product_category = collect[x][1]\n",
    "    star_rating = collect[x][2]\n",
    "    count = collect[x][3]\n",
    "    \n",
    "    # Saving these newly created variables into the 'scores' table \n",
    "    # (cf. amazon.sql) into the similarly named columns\n",
    "    my_cursor.execute(\"INSERT INTO \"\n",
    "                      \"scores(month,product_category,star_rating,count) \"\n",
    "                      \"values(%s, %s,%s, %s)\", \n",
    "                      (month,product_category,star_rating,count))\n",
    "\n",
    "\n",
    "    # Saving our database after every 1000'th row (not really 'saving' \n",
    "    # but let's say it does) so that even if our program crashes, we still\n",
    "    # don't lose all our progress. Also: printing our progress in percent, \n",
    "    # because it's pretty to see that number go up.\n",
    "    if(x%1000 == 0):\n",
    "        cnx.commit()\n",
    "        \n",
    "        progress = round(100 * x/total, 2)\n",
    "\n",
    "        print(\"Progress: %s%%\" % progress)\n",
    "        \n",
    "cnx.commit()\n",
    "\n",
    "print(\"DONE!\") \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Congratulations on making it through!! \n",
    "\n",
    "import webbrowser\n",
    "webbrowser.open('https://media.giphy.com/media/fxsqOYnIMEefC/giphy.gif')\n",
    "webbrowser.open('https://media.giphy.com/media/gFccuw5vFkc9trBiQ1/giphy.gif')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}